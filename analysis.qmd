---
title: Analysis
description: Here we provide a detailed analysis using more sophisticated statistics techniques.
toc: true
draft: false
---

![](images/MMRpics.jpeg)
As shown in the data page, we find out the unexpected positive growth of the maternal mortality rate over the States. In effort to figure out the possible reasons that has statistical significance causing this increasing trend, our group seek to explore if the annual spending on healthcare and the average age of pregnancy could contribute to the phenomenon. 

Our team focus on the following variables:

  - Demographic factors : race group, year, location
  - Other factors : spending on healthcare, average age of pregnancy.

We collected and cleaned the average spending of healthcare data and the average pregnancy age data. During the analyzing process, we notice a clear increasing trend in both the line plot of spending vs mmr and avaergae age vs mmr. Thus, we predict that a linear regression may provide a fit model for the deduction.



# Analysis
```{r load in the data, echo=FALSE}
suppressPackageStartupMessages(library(tidyverse))
load(here::here("dataset/mmr.RData"))
```

```{r figure 3, echo=FALSE}
mmr_data_clean |>
  group_by(year_id, location_name, race_group) |>
  filter(str_detect(location_name, "Census")) |> 
  summarize(mean = mean(val))|>
  ggplot(aes(x = year_id, y = mean)) +
  geom_line(aes(color = location_name)) + 
  facet_wrap(~race_group) + 
  labs(
    title = "Maternal Mortality by Census Region over Time, Faceted by Race",
    x = "Mean Maternal Mortality Ratio",
    y = "Year",
    color = "Census Region"
  )

```

Figure 3: The five graphs depict the trajectory of maternal mortality rates over a 20-year period, differentiated by racial categories and geographic regions in the United States.  These categories include Hispanic and any race, Non-Hispanic American Indian and Alaska Native, Non-Hispanic Asian and Native Hawaiian or Other Pacific Islander, Non-Hispanic Black, and Non-Hispanic White.  While there is an overarching increase in mortality rates across these racial demographics, the patterns of increase vary notably between them.

For the categories of Hispanic and any race, Non-Hispanic Asian and Native Hawaiian or Other Pacific Islander, and Non-Hispanic White, the mortality rates appear to be relatively stable, with slight fluctuations over the two decades.  The lines on the graph for these groups are relatively flat, suggesting less volatility in their yearly rates and possibly better outcomes when compared to the other categories.

Contrastingly, the Non-Hispanic Black category exhibits a pronounced upward trend in the Northeast and South regions, indicating a concerning rise in maternal mortality rates that surpasses the other racial groups in these areas.  This suggests that Non-Hispanic Black women in these regions are disproportionately affected by factors contributing to maternal mortality.

The Non-Hispanic American Indian and Alaska Native category demonstrates a significant increase in maternal mortality rates, particularly in the Midwest and West regions.  The sharp rise could be indicative of systemic health disparities or regional deficiencies in healthcare access or quality that particularly impact this demographic.

The observed regional differences, such as higher rates in the South and variations in the Midwest and West, might be influenced by a multitude of factors including healthcare infrastructure, socioeconomic status, accessibility of prenatal and postnatal care, and population density.  Regions with higher population densities might face different healthcare challenges than less densely populated areas, potentially affecting the availability and quality of care.


```{r figure 3.75, echo = FALSE}

load(here::here("dataset/phc_mmr.RData"))


df <- read_csv(here::here("dataset/phc_mmr_combined.csv"), col_types = cols(
  location_name= col_character(),
  year_id = col_double(),
  spending = col_double(),
  mean_val = col_double())) 

df |>
  group_by(year_id, location_name) |>
  filter(str_detect(location_name, "Census")) |>
  summarize(mean = mean(spending)) |>
  ggplot(aes(x = year_id, y = mean)) +
  geom_line(aes(color = location_name)) +
    labs(
    title = "Spending by Census Region over Time",
    y = "Mean Spending",
    x = "Year",
    color = "Census Region"
  )


```






```{r figure 5, echo=FALSE}
combined_data |> 
  ggplot(aes(x = spending, y = mean_val)) + 
  geom_point(aes(alpha = 0.01)) + 
  facet_wrap(~year_id) +  
  theme(legend.position = "none") + 
  labs(
    title = "Maternal Mortality v.s. Healthcare Spending by Year",
    x = "Spending",
    y = "MMR"
  )
```
Figure 5: Faceted MMR vs Spending
We then faceted the initial graph by year to show the trend over time. This reveals a few things. Firstly, when faceted by year the relationship no longer appears to have a positive correlation. The trend is hard to see without a regression however it does appear to be negatively correlated now. Additionally, the points for each state are spreading out. In the first few years, all the points are clumped together with lower values. However, as the years go by the points not only overall increase in value but move farther apart. This prompted us to do a standard deviation analysis to quantify this observed trend.







We describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You'll also reflect on next steps and further analysis.

The audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc. 

While the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points.
If you want you can link back to your blog posts or create separate pages with more details.

The style of this paper should aim to be that of an academic paper. 
I don't expect this to be of publication quality but you should keep that aim in mind.
Avoid using "we" too frequently, for example "We also found that ...". Describe your methodology and your findings but don't describe your whole process.


```{r figure 9, echo=FALSE}
library(ggplot2)
library(readr)

data <- read_csv("dataset/phc_mmr_with_age.csv")

data$spending <- as.numeric(data$spending)
data$AverageAge <- as.numeric(data$AverageAge)

ggplot(data, aes(x = spending, y = AverageAge, color = Region)) +
  geom_point(aes(size = 3), alpha = 0.6) +  
  geom_smooth(method = "lm", se = FALSE, aes(group = Region)) +  
  scale_size_identity() +  
  labs(title = "Bubble Chart with Fitted Lines by Region",
       x = "Healthcare Spending",
       y = "Average Age of Pregnancy",
       size = "Region") +
  theme_minimal() +
  facet_wrap(~Region)
  guides(color = guide_legend(title = "Region"))
```
```


## Note on Attribution

In general, you should try to provide links to relevant resources, especially those that helped you. You don't have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don't need a formal citation.

If you are directly quoting from a source, please make that clear. You can show quotes using `>` like this

```         
> To be or not to be.
```

> To be or not to be.


## Rubric: On this page

You will

-   Introduce what motivates your Data Analysis (DA)
    -   Which variables and relationships are you most interested in?
    -   What questions are you interested in answering?
    -   Provide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.
-   Modeling and Inference
    -   The page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.
    -   Explain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)
    -   Describe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.
-   Explain the flaws and limitations of your analysis
    -   Are there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?
-   Clarity Figures
    -   Are your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?
    -   Each figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)
    -   Default `lm` output and plots are typically not acceptable.
-   Clarity of Explanations
    -   How well do you explain each figure/result?
    -   Do you provide interpretations that suggest further analysis or explanations for observed phenomenon?
-   Organization and cleanliness.
    -   Make sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.
    -   This page should be self-contained, i.e. provide a description of the relevant data.


Notes from presentation:
cut: mmr by region not facetted since we also have facetted, STD graphs


Model Findings:
linear regression MMR-AGE
```{r, echo=FALSE}
# Load necessary packages
library(broom)
library(knitr)

data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)
data$AverageAge <- as.integer(data$AverageAge)
data$mean_val <- data$mean_val / 100
# Fit the simple linear regression model with AverageAge as the predictor
model <- lm(mean_val ~ AverageAge, data = data)

# Tidy the regression coefficients
coefficients_table <- tidy(model)

# Gather model statistics
model_summary <- summary(model)
fit_statistics <- data.frame(
  Metric = c("R-squared", "Adjusted R-squared", "Sigma", "F-statistic", "p-value", "Degrees of Freedom"),
  Value = c(
    model_summary$r.squared,
    model_summary$adj.r.squared,
    model_summary$sigma,
    model_summary$fstatistic[1],
    pf(model_summary$fstatistic[1], model_summary$fstatistic[2], model_summary$fstatistic[3], lower.tail = FALSE),
    model_summary$fstatistic[2]
  )
)

# Output the coefficients table
kable(
  coefficients_table,
  col.names = c("Term", "Estimate", "Standard Error", "t-Statistic", "p-Value"),
  caption = "Regression Coefficients Table"
)

# Output the model fit statistics table
kable(
  fit_statistics,
  col.names = c("Metric", "Value"),
  caption = "Model Fit Statistics Table"
)

```
**Regression Coefficients Table**: 
  Estimate: This represents the estimated coefficient for each term in the model. For the intercept, it is approximately -0.2916, suggesting that when the "AverageAge" is zero, the MMR would be around -0.2916, assuming that zero is a meaningful and valid value for "AverageAge". The coefficient for "AverageAge" is approximately 0.0216, implying that for every one-unit increase in "AverageAge", the MMR increases by about 0.0216 units.
  
  Standard Error: This measures the average amount that the estimated coefficients vary from the actual average value of our response variable. The standard error for the intercept is around 0.1061 and for "AverageAge" it is about 0.0040. Smaller standard errors indicate more precise estimates.
  
  t-Statistic: This is calculated as the estimate divided by the standard error, providing a measure of how many standard errors the coefficient is away from zero. A high absolute value of the t-statistic indicates it is unlikely that the coefficient is zero by chance (assuming the null hypothesis is that the coefficient is zero). Here, the t-statistics are -2.748 for the intercept and 5.340 for "AverageAge".
  
  p-Value: This shows the probability of observing any value equal to or more extreme than the observed one, under the null hypothesis. A smaller p-value indicates stronger evidence against the null hypothesis. The p-value for the intercept is 0.0061, suggesting that there is a statistically significant relationship at common significance levels (e.g., 0.05, 0.01). For "AverageAge", the p-value is extremely small (0.0000001), indicating very strong evidence against the null hypothesis and suggesting that "AverageAge" is a significant predictor of the outcome. 
  
  Conclusion: Both the intercept and the variable "AverageAge" are statistically significant predictors in this model. "AverageAge" particularly shows a strong positive association with the dependent variable, with very high statistical significance.




correlation matrix
```{r, echo=FALSE}

# Load the dataset from the uploaded file
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing values in important columns
data <- na.omit(data)

# Convert AverageAge to integer
data$AverageAge <- as.integer(data$AverageAge)

# Convert Region to a factor (if it's not already)
data$Region <- as.factor(data$Region)
data$race_group <- as.factor(data$Region)

# Generate dummy variables for Region
region_dummies <- model.matrix(~ Region - 1, data)

# Combine the numeric data with the dummy variables
numeric_data <- cbind(data[, sapply(data, is.numeric)], region_dummies)

# Compute the correlation matrix
correlation_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Create a formal table using kable
kable(correlation_matrix, caption = "Correlation Matrix")
```

MLR

```{r, echo=FALSE}
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)
print(levels(data$Region))
data <- na.omit(data)

# Convert AverageAge to integer
data$AverageAge <- as.integer(data$AverageAge)

# Convert Region to a factor and exclude one level to avoid singularities
data$Region <- factor(data$Region)

# Generate dummy variables for Region excluding one level
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the numeric data with the dummy variables
mlr_data <- cbind(data[, c("AverageAge", "spending", "mean_val")], region_dummies)

# Fit the multiple linear regression model
mlr_model <- lm(mean_val ~ ., data = mlr_data)

# Get the summary and convert it to a data frame
summary_model <- summary(mlr_model)
coef_table <- as.data.frame(summary_model$coefficients)

# Display the coefficients in a Kable table
kable(coef_table, caption = "Multiple Linear Regression Coefficients")
```

```{r, echo=FALSE}
# Load the data
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing values in critical columns
data <- na.omit(data)

# Convert AverageAge to integer
data$AverageAge <- as.integer(data$AverageAge)

# Convert Region to a factor and exclude one level to avoid singularities
data$Region <- factor(data$Region)

# Generate dummy variables for Region excluding one level
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the numeric data with the dummy variables
mlr_data <- cbind(data[, c("AverageAge", "spending", "mean_val")], region_dummies)

# Fit the multiple linear regression model
mlr_model <- lm(mean_val ~ ., data = mlr_data)

# Get the summary and coefficients table
summary_model <- summary(mlr_model)
coef_table <- as.data.frame(summary_model$coefficients)

# Extract additional statistics
r_squared <- summary_model$r.squared
adj_r_squared <- summary_model$adj.r.squared
sigma <- summary_model$sigma
f_statistic <- summary_model$fstatistic[1]
f_pvalue <- pf(summary_model$fstatistic[1], summary_model$fstatistic[2], summary_model$fstatistic[3], lower.tail = FALSE)
df <- summary_model$df[1]
log_lik <- logLik(mlr_model)
aic <- AIC(mlr_model)
bic <- BIC(mlr_model)
deviance <- deviance(mlr_model)
df_residual <- df.residual(mlr_model)
nobs <- nobs(mlr_model)

# Create a summary table
model_stats <- data.frame(
  r.squared = r_squared,
  adj.r.squared = adj_r_squared,
  sigma = sigma,
  statistic = f_statistic,
  p.value = f_pvalue,
  df = df,
  logLik = as.numeric(log_lik),
  AIC = aic,
  BIC = bic,
  deviance = deviance,
  df.residual = df_residual,
  nobs = nobs
)
# Display the model statistics in a Kable table
kable(model_stats, caption = "Multiple Linear Regression Model Statistics")

```

Residual Plot
```{r, echo=FALSE}
# Load the data
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing values in critical columns
data <- na.omit(data)

# Convert AverageAge to integer
data$AverageAge <- as.integer(data$AverageAge)

# Convert Region to a factor and exclude one level to avoid singularities
data$Region <- factor(data$Region)

# Generate dummy variables for Region excluding one level
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the numeric data with the dummy variables
mlr_data <- cbind(data[, c("AverageAge", "spending", "mean_val")], region_dummies)

# Fit the multiple linear regression model
mlr_model <- lm(mean_val ~ ., data = mlr_data)

# Create residual plots
# Plot Residuals vs Fitted Values
par(mfrow = c(1, 1))  # Single plot layout
plot(mlr_model, which = 1)  # Residuals vs Fitted

```
QQ-Plot
```{r, echo=FALSE}
# Load the data
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing values in critical columns
data <- data[complete.cases(data[c("AverageAge", "spending", "mean_val", "Region")]), ]

# Convert AverageAge to numeric and handle missing or non-numeric values
data$AverageAge <- as.numeric(data$AverageAge)

# Convert Region to a factor, ensuring all values are present
data$Region <- factor(data$Region)

# Generate dummy variables for Region, excluding the first level to avoid singularities
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the numeric data with the dummy variables
mlr_data <- cbind(data[, c("AverageAge", "spending", "mean_val")], region_dummies)

# Fit the multiple linear regression model
mlr_model <- lm(mean_val ~ ., data = mlr_data)

# Generate QQ plot
qqnorm(resid(mlr_model))
qqline(resid(mlr_model), col = "red")
```


LOG-LOG
```{r, echo=FALSE}
# Load necessary libraries
library(readr)
library(knitr)

# Load the dataset
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing or invalid values in critical columns
data <- data[data$AverageAge > 0 & data$spending > 0 & data$mean_val > 0 & !is.na(data$Region), ]

# Convert AverageAge to numeric for log transformation
data$AverageAge <- as.numeric(data$AverageAge)

# Apply log transformation to the variables
data$log_AverageAge <- log(data$AverageAge)
data$log_spending <- log(data$spending)
data$log_mean_val <- log(data$mean_val)

# Convert Region to a factor
data$Region <- factor(data$Region)

# Generate dummy variables for Region, excluding the first level to avoid singularities
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the log-transformed data with the dummy variables
log_mlr_data <- cbind(data[, c("log_AverageAge", "log_spending", "log_mean_val")], region_dummies)

# Fit the log-log regression model
log_mlr_model <- lm(log_mean_val ~ ., data = log_mlr_data)

# Get the summary and convert it to a data frame
log_summary_model <- summary(log_mlr_model)
log_coef_table <- as.data.frame(log_summary_model$coefficients)

# Display the coefficients in a Kable table
kable(log_coef_table, caption = "Log-Log Multiple Linear Regression Coefficients")

```

```{r}
# Load necessary libraries
library(readr)
library(knitr)

# Load the dataset
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing or invalid values in critical columns
data <- data[data$AverageAge > 0 & data$spending > 0 & data$mean_val > 0 & !is.na(data$Region), ]

# Convert AverageAge to numeric for log transformation
data$AverageAge <- as.numeric(data$AverageAge)

# Apply log transformation to the variables
data$log_AverageAge <- log(data$AverageAge)
data$log_spending <- log(data$spending)
data$log_mean_val <- log(data$mean_val)

# Convert Region to a factor
data$Region <- factor(data$Region)

# Generate dummy variables for Region, excluding the first level to avoid singularities
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the log-transformed data with the dummy variables
log_mlr_data <- cbind(data[, c("log_AverageAge", "log_spending", "log_mean_val")], region_dummies)

# Fit the log-log regression model
log_mlr_model <- lm(log_mean_val ~ ., data = log_mlr_data)

# Extract model summary statistics
log_summary_model <- summary(log_mlr_model)
model_stats <- data.frame(
  R_squared = log_summary_model$r.squared,
  Adj_R_squared = log_summary_model$adj.r.squared,
  Sigma = log_summary_model$sigma,
  Statistic = log_summary_model$fstatistic[1],
  P_value = pf(log_summary_model$fstatistic[1], log_summary_model$fstatistic[2], log_summary_model$fstatistic[3], lower.tail = FALSE),
  DF = log_summary_model$df[1],
  LogLik = as.numeric(logLik(log_mlr_model)),
  AIC = AIC(log_mlr_model),
  BIC = BIC(log_mlr_model),
  Deviance = deviance(log_mlr_model),
  DF_Residual = df.residual(log_mlr_model),
  Nobs = nobs(log_mlr_model)
)

# Display the model statistics in a Kable table
kable(model_stats, caption = "Log-Log Regression Model Statistics")

```




QQ and residual
```{r}
# Load necessary libraries
library(readr)
library(knitr)
library(ggplot2)

# Load the dataset
data <- read_csv("dataset/phc_mmr_with_age.csv", show_col_types = FALSE)

# Remove rows with missing or invalid values in critical columns
data <- data[data$AverageAge > 0 & data$spending > 0 & data$mean_val > 0 & !is.na(data$Region), ]

# Convert AverageAge to numeric for log transformation
data$AverageAge <- as.numeric(data$AverageAge)

# Apply log transformation to the variables
data$log_AverageAge <- log(data$AverageAge)
data$log_spending <- log(data$spending)
data$log_mean_val <- log(data$mean_val)

# Convert Region to a factor
data$Region <- factor(data$Region)

# Generate dummy variables for Region, excluding the first level to avoid singularities
region_dummies <- model.matrix(~ Region, data)[, -1]

# Combine the log-transformed data with the dummy variables
log_mlr_data <- cbind(data[, c("log_AverageAge", "log_spending", "log_mean_val")], region_dummies)

# Fit the log-log regression model
log_mlr_model <- lm(log_mean_val ~ ., data = log_mlr_data)

# Residuals vs Fitted plot
ggplot(data.frame(fitted = fitted(log_mlr_model), residuals = resid(log_mlr_model)), aes(fitted, residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted Plot", x = "Fitted values", y = "Residuals") +
  theme_minimal()

# QQ plot
qqnorm(resid(log_mlr_model))
qqline(resid(log_mlr_model), col = "red")

```

